{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc7f0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  184.84 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc790>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  496.08 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9160>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  423.75 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9100>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  336.10 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9130>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  854.57 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9970>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.22 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc7c0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  1.27 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9250>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  3.03 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc4c0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  4.27 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd93a0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  5.74 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd93d0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  11.45 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9be0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  17.51 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcac0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  22.58 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9190>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  44.85 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9e50>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  68.61 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9790>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  163.21 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd93d0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  372.24 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9220>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  535.45 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecc7fd0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  252.72 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcbb0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  886.37 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9a00>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.13 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9970>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  538.10 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd93d0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  1.38 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9a90>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.97 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd94f0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  2.56 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcac0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  5.34 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9880>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  8.00 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9070>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  269.58 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd91f0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  754.52 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9bb0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.04 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcee0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  498.79 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9220>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  1.40 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9b80>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.93 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9fd0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  1.30 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9af0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  2.79 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9160>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  4.16 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc7c0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  5.04 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9370>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  10.75 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9100>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  16.12 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9fa0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  2.33 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd91f0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  5.11 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9bb0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  7.60 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbc4c0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  9.20 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd99a0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  18.93 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9760>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  28.49 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9520>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  36.18 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9640>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  74.00 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9190>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  112.18 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcfa0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  87.34 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9f70>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  220.45 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9bb0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  383.39 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9160>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  231.88 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9d90>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  552.13 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9f40>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  822.38 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcbe0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  756.08 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9670>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  1.76 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9100>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  2.56 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9df0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  2.94 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9790>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  6.20 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9b80>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  9.22 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcc10>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  11.67 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9e20>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  23.39 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd94f0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  35.44 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9ee0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  120.40 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9a00>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  350.59 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9e80>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  461.54 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcbb0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  169.51 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9f40>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  464.82 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9370>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  648.77 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd96d0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  329.13 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9f10>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  733.94 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd90a0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.08 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecc76a0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  1.30 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd99a0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  2.62 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9af0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  3.97 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9250>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  176.96 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9df0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  433.22 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9ee0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  620.58 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecc76a0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  291.61 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9220>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  724.50 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd94f0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  1.04 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd96d0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  657.84 us\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9190>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  1.44 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9c70>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  2.11 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecc76a0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  2.65 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9070>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  5.29 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd91f0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  7.97 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9bb0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  1.24 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd94c0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  2.87 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9190>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  4.17 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecbcac0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  4.80 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd95b0>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  10.20 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd90d0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  15.20 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9df0>\n",
      "fn_amp(*inputs, **kwinputs)\n",
      "  18.90 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd9e80>\n",
      "y.backward(grad, retain_graph=True)\n",
      "  38.70 ms\n",
      "  1 measurement, 100 runs , 10 threads\n",
      "FlashAttention - Forward + Backward pass\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fbd4ecd94c0>\n",
      "f(grad, *inputs, **kwinputs)\n",
      "  58.25 ms\n",
      "  1 measurement, 100 runs , 10 threads\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward, benchmark_combined\n",
    "from flash_attn.bert_padding import unpad_input, pad_input\n",
    "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
    "import tqdm \n",
    "\n",
    "def attention_ref(qkv, attn_mask, dropout_p, upcast=False, causal=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        attn_mask: (batch_size, seqlen)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "        attention: softmax after dropout\n",
    "    \"\"\"\n",
    "    q, k, v = (qkv.float() if upcast else qkv).unbind(dim=2)\n",
    "    seqlen = qkv.shape[1]\n",
    "    d = qkv.shape[-1]\n",
    "    scores = torch.einsum('bthd,bshd->bhts', q, k / math.sqrt(d))\n",
    "    scores.masked_fill_(rearrange(~attn_mask, 'b s -> b 1 1 s'), float('-inf'))\n",
    "    if causal:\n",
    "        causal_mask = torch.triu(torch.ones(seqlen, seqlen, dtype=torch.bool, device=qkv.device), 1)\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    # return output.to(dtype=qkv.dtype), attention.to(dtype=qkv.dtype)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "configurations = [\n",
    "    (1, 40, 128, 512),\n",
    "    (1, 40, 128, 1024),\n",
    "    (1, 40, 128, 2048),\n",
    "    (1, 40, 128, 4096),\n",
    "    (1, 40, 128, 8192),\n",
    "    (1, 8, 128, 1536),\n",
    "    (1, 8, 128, 2048),\n",
    "    (1, 8, 128, 3072),\n",
    "    (1, 8, 128, 6144),\n",
    "    (1, 16, 128, 1536),\n",
    "    (1, 16, 128, 2048),\n",
    "    (1, 16, 128, 3072),\n",
    "    (1, 16, 128, 6144),\n",
    "    (1, 64, 128, 2048),\n",
    "    (1, 64, 128, 4096),\n",
    "    (1, 64, 128, 8192)\n",
    "]\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "causal_vals = [False, True]\n",
    "total_iterations = len(causal_vals)* len(configurations)\n",
    "# progress_bar = tqdm(total=total_iterations, desc=\"Processing Configurations\")\n",
    "torch.manual_seed(0)\n",
    "repeats = 100\n",
    "batch_size = 64\n",
    "nheads = 16\n",
    "seqlen = 1024\n",
    "n = 1024\n",
    "d = n // nheads\n",
    "dropout_p = 0.0\n",
    "\n",
    "dtype = torch.float16\n",
    "device = 'cuda'\n",
    "for causal in causal_vals: # This loop may not be necessary if you're only using headdim=128\n",
    "    for config_4 in configurations:\n",
    "        batch_size, nheads, d, seqlen = config_4\n",
    "        config = (causal, batch_size, nheads, d, seqlen)\n",
    "        n=nheads*d\n",
    "        x = torch.randn(batch_size, seqlen, n, device='cuda', dtype=dtype, requires_grad=True)\n",
    "        Wqkv = torch.nn.Linear(nheads * d, 3 * nheads * d, device=device, dtype=dtype)\n",
    "\n",
    "        lengths = torch.randint(seqlen - 20, seqlen, (batch_size, 1), device='cuda')\n",
    "        attention_mask_bool = repeat(torch.arange(seqlen, device='cuda'), 's -> b s', b=batch_size) < lengths\n",
    "        attention_mask = torch.zeros(batch_size, seqlen, device='cuda', dtype=dtype)\n",
    "        attention_mask[~attention_mask_bool] = -10000.0\n",
    "        attention_mask = rearrange(attention_mask, 'b s -> b 1 1 s')\n",
    "\n",
    "        x_unpad, indices, cu_seqlens, max_seqlen_in_batch = unpad_input(x, attention_mask_bool)\n",
    "        qkv_unpad = rearrange(Wqkv(x_unpad), 'nnz (t h d) -> nnz t h d', t=3,\n",
    "                            h=nheads).detach().requires_grad_()\n",
    "        qkv = rearrange(Wqkv(x), 'b s (t h d) -> b s t h d', t=3, h=nheads).detach().requires_grad_()\n",
    "\n",
    "        fn = lambda qkv_unpad: flash_attn_unpadded_qkvpacked_func(\n",
    "            qkv_unpad, cu_seqlens, max_seqlen_in_batch, dropout_p, causal=causal\n",
    "        )\n",
    "        FA1=benchmark_all(fn, qkv_unpad, repeats=repeats, desc='FlashAttention')\n",
    "        fn = lambda qkv: attention_ref(qkv, attention_mask_bool, dropout_p, causal=causal)\n",
    "        # pytorch=benchmark_all(fn, qkv, repeats=repeats, desc='PyTorch Standard Attention')\n",
    "        time_f[config, \"Flash1\"] = FA1[0][1].mean\n",
    "        time_b[config, \"Flash1\"] = FA1[1][1].mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 将 (config, method) 结构转换为分开的多列数据\n",
    "df_time_f = pd.DataFrame([(causal, batch_size, nheads, headdim, seqlen, method, t) \n",
    "                          for ((causal, batch_size, nheads, headdim, seqlen), method), t in time_f.items()], \n",
    "                          columns=['Causal', 'BatchSize','nHeads','HeadDim', 'SeqLen', 'Method', 'Time F'])\n",
    "\n",
    "df_time_b = pd.DataFrame([(causal, batch_size, nheads, headdim, seqlen, method, t) \n",
    "                          for ((causal, batch_size, nheads, headdim, seqlen), method), t in time_b.items()], \n",
    "                          columns=['Causal',  'BatchSize','nHeads','HeadDim', 'SeqLen', 'Method', 'Time B'])\n",
    "df_time_f = df_time_f[df_time_f['Method'] != 'Triton']\n",
    "df_time_b = df_time_b[df_time_b['Method'] != 'Triton']\n",
    "# 保存 DataFrame 到 Excel 文件\n",
    "with pd.ExcelWriter('times11.xlsx') as writer:\n",
    "    df_time_f.to_excel(writer, sheet_name='Forward Times', index=False)\n",
    "    df_time_b.to_excel(writer, sheet_name='Backward Times', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036811568463842076"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FA1[0][1].mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
