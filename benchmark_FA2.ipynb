{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Configurations: 100%|██████████| 32/32 [01:25<00:00,  6.34s/it]"
     ]
    }
   ],
   "source": [
    "# Install the newest triton version with\n",
    "# pip install \"git+https://github.com/openai/triton.git#egg=triton&subdirectory=python\"\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from einops import rearrange, repeat\n",
    "import pandas as pd\n",
    "from flash_attn.utils.benchmark import benchmark_all, benchmark_forward, benchmark_backward\n",
    "from flash_attn.utils.benchmark import benchmark_fwd_bwd, benchmark_combined\n",
    "from tqdm import tqdm\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "try:\n",
    "    from triton.ops.flash_attention import attention as attention_triton\n",
    "except ImportError:\n",
    "    attention_triton = None\n",
    "\n",
    "try:\n",
    "    import xformers.ops as xops\n",
    "except ImportError:\n",
    "    xops = None\n",
    "\n",
    "\n",
    "def flops(batch, seqlen, headdim, nheads, causal, mode=\"fwd\"):\n",
    "    assert mode in [\"fwd\", \"bwd\", \"fwd_bwd\"]\n",
    "    f = 4 * batch * seqlen**2 * nheads * headdim // (2 if causal else 1)\n",
    "    return f if mode == \"fwd\" else (2.5 * f if mode == \"bwd\" else 3.5 * f)\n",
    "\n",
    "def efficiency(flop, time):\n",
    "    return (flop / time / 10**12) if not math.isnan(time) else 0.0\n",
    "\n",
    "\n",
    "def attention_pytorch(qkv, dropout_p=0.0, causal=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        qkv: (batch_size, seqlen, 3, nheads, head_dim)\n",
    "        dropout_p: float\n",
    "    Output:\n",
    "        output: (batch_size, seqlen, nheads, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen, _, nheads, d = qkv.shape\n",
    "    q, k, v = qkv.unbind(dim=2)\n",
    "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
    "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
    "    softmax_scale = 1.0 / math.sqrt(d)\n",
    "    # Preallocate attn_weights for `baddbmm`\n",
    "    scores = torch.empty(batch_size * nheads, seqlen, seqlen, dtype=qkv.dtype, device=qkv.device)\n",
    "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
    "                       '(b h) t s -> b h t s', h=nheads)\n",
    "    if causal:\n",
    "        # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "        # So we have to construct the mask in float\n",
    "        causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "        # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\n",
    "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    attention_drop = F.dropout(attention, dropout_p)\n",
    "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    return output.to(dtype=qkv.dtype)\n",
    "\n",
    "\n",
    "def time_fwd_bwd(func, *args, **kwargs):\n",
    "    time_f, time_b = benchmark_fwd_bwd(func, *args, **kwargs)\n",
    "    return time_f[1].mean, time_b[1].mean\n",
    "\n",
    "repeats = 100\n",
    "device = 'cuda'\n",
    "dtype = torch.float16\n",
    "\n",
    "# bs_seqlen_vals = [(32, 512), (16, 1024), (8, 2048), (4, 4096), (2, 8192), (1, 16384)]\n",
    "causal_vals = [False, True]\n",
    "# headdim_vals = [64, 128]\n",
    "# dim = 2048\n",
    "dropout_p = 0.0\n",
    "\n",
    "methods = ([\"Flash2\", \"Pytorch\"]\n",
    "           + ([\"Triton\"] if attention_triton is not None else [])\n",
    "           + ([\"xformers.c\"] if xops is not None else [])\n",
    "           + ([\"xformers.f\"] if xops is not None else []))\n",
    "configurations = [\n",
    "    (1, 40, 128, 512),\n",
    "    (1, 40, 128, 1024),\n",
    "    (1, 40, 128, 2048),\n",
    "    (1, 40, 128, 4096),\n",
    "    (1, 40, 128, 8192),\n",
    "    (1, 8, 128, 1536),\n",
    "    (1, 8, 128, 2048),\n",
    "    (1, 8, 128, 3072),\n",
    "    (1, 8, 128, 6144),\n",
    "    (1, 16, 128, 1536),\n",
    "    (1, 16, 128, 2048),\n",
    "    (1, 16, 128, 3072),\n",
    "    (1, 16, 128, 6144),\n",
    "    (1, 64, 128, 2048),\n",
    "    (1, 64, 128, 4096),\n",
    "    (1, 64, 128, 8192)\n",
    "]\n",
    "\n",
    "total_iterations = len(causal_vals)* len(configurations)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Processing Configurations\")\n",
    "time_f = {}\n",
    "time_b = {}\n",
    "time_f_b = {}\n",
    "speed_f = {}\n",
    "speed_b = {}\n",
    "speed_f_b = {}\n",
    "for causal in causal_vals: # This loop may not be necessary if you're only using headdim=128\n",
    "    for config_4 in configurations:\n",
    "        batch_size, nheads, headdim, seqlen = config_4\n",
    "        config = (causal, batch_size, nheads, headdim, seqlen)\n",
    "        qkv = torch.randn(batch_size, seqlen, 3, nheads, headdim, device=device, dtype=dtype,\n",
    "                            requires_grad=True)\n",
    "        f, b = time_fwd_bwd(\n",
    "            flash_attn_qkvpacked_func, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "        )\n",
    "        time_f[config, \"Flash2\"] = f\n",
    "        time_b[config, \"Flash2\"] = b\n",
    "\n",
    "        try:\n",
    "            qkv = qkv.detach().requires_grad_(True)\n",
    "            f, b = time_fwd_bwd(\n",
    "                attention_pytorch, qkv, dropout_p, causal=causal, repeats=repeats, verbose=False\n",
    "            )\n",
    "        except:  # Skip if OOM\n",
    "            f, b = float('nan'), float('nan')\n",
    "        time_f[config, \"Pytorch\"] = f\n",
    "        time_b[config, \"Pytorch\"] = b\n",
    "\n",
    "        if attention_triton is not None:\n",
    "            q, k, v = [torch.randn(batch_size, nheads, seqlen, headdim, device=device, dtype=dtype,\n",
    "                                requires_grad=True) for _ in range(3)]\n",
    "            # Try both values of sequence_parallel and pick the faster one\n",
    "            try:\n",
    "                f, b = time_fwd_bwd(\n",
    "                    attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "                    False, repeats=repeats, verbose=False\n",
    "                )\n",
    "            except:\n",
    "                f, b = float('nan'), float('inf')\n",
    "            try:\n",
    "                _, b0 = time_fwd_bwd(\n",
    "                    attention_triton, q, k, v, causal, headdim**(-0.5),\n",
    "                    True, repeats=repeats, verbose=False\n",
    "                )\n",
    "            except:\n",
    "                b0 = float('inf')\n",
    "            time_f[config, \"Triton\"] = f\n",
    "            time_b[config, \"Triton\"] = min(b, b0) if min(b, b0) < float('inf') else float('nan')\n",
    "\n",
    "        if xops is not None:\n",
    "            q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                                requires_grad=True) for _ in range(3)]\n",
    "            f, b = time_fwd_bwd(\n",
    "                xops.memory_efficient_attention, q, k, v,\n",
    "                attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "                op=(xops.fmha.cutlass.FwOp, xops.fmha.cutlass.BwOp)\n",
    "            )\n",
    "            time_f[config, \"xformers.c\"] = f\n",
    "            time_b[config, \"xformers.c\"] = b\n",
    "\n",
    "        if xops is not None:\n",
    "            q, k, v = [torch.randn(batch_size, seqlen, nheads, headdim, device=device, dtype=dtype,\n",
    "                                requires_grad=True) for _ in range(3)]\n",
    "            f, b = time_fwd_bwd(\n",
    "                xops.memory_efficient_attention, q, k, v,\n",
    "                attn_bias=xops.LowerTriangularMask() if causal else None,\n",
    "                op=(xops.fmha.flash.FwOp, xops.fmha.flash.BwOp)\n",
    "            )\n",
    "            time_f[config, \"xformers.f\"] = f\n",
    "            time_b[config, \"xformers.f\"] = b\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # print(f\"### causal={causal}, headdim={headdim}, batch_size={batch_size}, seqlen={seqlen} ###\")\n",
    "        # for method in methods:\n",
    "        #     time_f_b[config, method] = time_f[config, method] + time_b[config, method]\n",
    "        #     speed_f[config, method] = efficiency(\n",
    "        #         flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd\"),\n",
    "        #         time_f[config, method]\n",
    "        #     )\n",
    "        #     speed_b[config, method] = efficiency(\n",
    "        #         flops(batch_size, seqlen, headdim, nheads, causal, mode=\"bwd\"),\n",
    "        #         time_b[config, method]\n",
    "        #     )\n",
    "        #     speed_f_b[config, method] = efficiency(\n",
    "        #         flops(batch_size, seqlen, headdim, nheads, causal, mode=\"fwd_bwd\"),\n",
    "        #         time_f_b[config, method]\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"{method} fwd: {speed_f[config, method]:.2f} TFLOPs/s, \"\n",
    "        #         f\"bwd: {speed_b[config, method]:.2f} TFLOPs/s, \"\n",
    "        #         f\"fwd + bwd: {speed_f_b[config, method]:.2f} TFLOPs/s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Configurations: 100%|██████████| 32/32 [01:25<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((False, 1, 40, 128, 512), 'Flash2'): 7.398700341582298e-05,\n",
       " ((False, 1, 40, 128, 512), 'Pytorch'): 0.00018832825124263765,\n",
       " ((False, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 1024), 'Flash2'): 0.0001949380338191986,\n",
       " ((False, 1, 40, 128, 1024), 'Pytorch'): 0.0005280192196369171,\n",
       " ((False, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 2048), 'Flash2'): 0.00044373203068971635,\n",
       " ((False, 1, 40, 128, 2048), 'Pytorch'): 0.0015537716820836066,\n",
       " ((False, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 4096), 'Flash2'): 0.00178089814260602,\n",
       " ((False, 1, 40, 128, 4096), 'Pytorch'): 0.0059799677319824695,\n",
       " ((False, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 8192), 'Flash2'): 0.007277089785784483,\n",
       " ((False, 1, 40, 128, 8192), 'Pytorch'): 0.023374582156538964,\n",
       " ((False, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 1536), 'Flash2'): 7.451500743627549e-05,\n",
       " ((False, 1, 8, 128, 1536), 'Pytorch'): 0.0002345100976526737,\n",
       " ((False, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 2048), 'Flash2'): 0.00014221351593732834,\n",
       " ((False, 1, 8, 128, 2048), 'Pytorch'): 0.00034161580726504325,\n",
       " ((False, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 3072), 'Flash2'): 0.000214122012257576,\n",
       " ((False, 1, 8, 128, 3072), 'Pytorch'): 0.000809457041323185,\n",
       " ((False, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 6144), 'Flash2'): 0.0008692327700555324,\n",
       " ((False, 1, 8, 128, 6144), 'Pytorch'): 0.0030595683120191097,\n",
       " ((False, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 1536), 'Flash2'): 0.00012004587799310684,\n",
       " ((False, 1, 16, 128, 1536), 'Pytorch'): 0.00043376151472330094,\n",
       " ((False, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 2048), 'Flash2'): 0.0002387201227247715,\n",
       " ((False, 1, 16, 128, 2048), 'Pytorch'): 0.0006345830112695693,\n",
       " ((False, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 3072), 'Flash2'): 0.0004338732920587063,\n",
       " ((False, 1, 16, 128, 3072), 'Pytorch'): 0.0015640468709170819,\n",
       " ((False, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 6144), 'Flash2'): 0.0017551548406481742,\n",
       " ((False, 1, 16, 128, 6144), 'Pytorch'): 0.006032583843916655,\n",
       " ((False, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 2048), 'Flash2'): 0.0007717288658022881,\n",
       " ((False, 1, 64, 128, 2048), 'Pytorch'): 0.002488524131476879,\n",
       " ((False, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 4096), 'Flash2'): 0.0029259537532925605,\n",
       " ((False, 1, 64, 128, 4096), 'Pytorch'): 0.009728931300342082,\n",
       " ((False, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 8192), 'Flash2'): 0.011624757833778858,\n",
       " ((False, 1, 64, 128, 8192), 'Pytorch'): 0.037755873911082746,\n",
       " ((False, 1, 64, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 512), 'Flash2'): 6.22955895960331e-05,\n",
       " ((True, 1, 40, 128, 512), 'Pytorch'): 0.00027652382850646974,\n",
       " ((True, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 1024), 'Flash2'): 0.0001347656361758709,\n",
       " ((True, 1, 40, 128, 1024), 'Pytorch'): 0.0006315072998404503,\n",
       " ((True, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 2048), 'Flash2'): 0.00030866755172610284,\n",
       " ((True, 1, 40, 128, 2048), 'Pytorch'): 0.0025087839551270006,\n",
       " ((True, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 4096), 'Flash2'): 0.0010923170484602451,\n",
       " ((True, 1, 40, 128, 4096), 'Pytorch'): 0.010051079392433167,\n",
       " ((True, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 8192), 'Flash2'): 0.004097110796719789,\n",
       " ((True, 1, 40, 128, 8192), 'Pytorch'): 0.03983450936153531,\n",
       " ((True, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 1536), 'Flash2'): 6.884951144456864e-05,\n",
       " ((True, 1, 8, 128, 1536), 'Pytorch'): 0.00035479912534356115,\n",
       " ((True, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 2048), 'Flash2'): 0.0001222752407193184,\n",
       " ((True, 1, 8, 128, 2048), 'Pytorch'): 0.0005747934617102146,\n",
       " ((True, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 3072), 'Flash2'): 0.00016971420496702193,\n",
       " ((True, 1, 8, 128, 3072), 'Pytorch'): 0.001374400295317173,\n",
       " ((True, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 6144), 'Flash2'): 0.000600740872323513,\n",
       " ((True, 1, 8, 128, 6144), 'Pytorch'): 0.005292902141809464,\n",
       " ((True, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 1536), 'Flash2'): 0.00011988669633865356,\n",
       " ((True, 1, 16, 128, 1536), 'Pytorch'): 0.0006521859392523765,\n",
       " ((True, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 2048), 'Flash2'): 0.00016269100829958917,\n",
       " ((True, 1, 16, 128, 2048), 'Pytorch'): 0.0010558013059198857,\n",
       " ((True, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 3072), 'Flash2'): 0.0003065974824130535,\n",
       " ((True, 1, 16, 128, 3072), 'Pytorch'): 0.00256704019382596,\n",
       " ((True, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 6144), 'Flash2'): 0.0010265797562897206,\n",
       " ((True, 1, 16, 128, 6144), 'Pytorch'): 0.010014307480305433,\n",
       " ((True, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 2048), 'Flash2'): 0.0004717133939266205,\n",
       " ((True, 1, 64, 128, 2048), 'Pytorch'): 0.00395339461043477,\n",
       " ((True, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 4096), 'Flash2'): 0.001720838099718094,\n",
       " ((True, 1, 64, 128, 4096), 'Pytorch'): 0.01602940207347274,\n",
       " ((True, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 8192), 'Flash2'): 0.0064673137851059435,\n",
       " ((True, 1, 64, 128, 8192), 'Pytorch'): 0.06325799949467183,\n",
       " ((True, 1, 64, 128, 8192), 'Triton'): nan}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((False, 1, 40, 128, 512), 'Flash2'): 0.00023322777822613717,\n",
       " ((False, 1, 40, 128, 512), 'Pytorch'): 0.000425503458827734,\n",
       " ((False, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 1024), 'Flash2'): 0.0006006010062992573,\n",
       " ((False, 1, 40, 128, 1024), 'Pytorch'): 0.000992963407188654,\n",
       " ((False, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 2048), 'Flash2'): 0.001444813795387745,\n",
       " ((False, 1, 40, 128, 2048), 'Pytorch'): 0.003181739915162325,\n",
       " ((False, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 4096), 'Flash2'): 0.005307868830859661,\n",
       " ((False, 1, 40, 128, 4096), 'Pytorch'): 0.011935706213116647,\n",
       " ((False, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 8192), 'Flash2'): 0.020440635737031698,\n",
       " ((False, 1, 40, 128, 8192), 'Pytorch'): 0.043976778630167246,\n",
       " ((False, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 1536), 'Flash2'): 0.00019406640902161598,\n",
       " ((False, 1, 8, 128, 1536), 'Pytorch'): 0.00044881651178002357,\n",
       " ((False, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 2048), 'Flash2'): 0.0004031822271645069,\n",
       " ((False, 1, 8, 128, 2048), 'Pytorch'): 0.0007375261560082436,\n",
       " ((False, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 3072), 'Flash2'): 0.0006529037468135357,\n",
       " ((False, 1, 8, 128, 3072), 'Pytorch'): 0.001475837416946888,\n",
       " ((False, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 6144), 'Flash2'): 0.002525606956332922,\n",
       " ((False, 1, 8, 128, 6144), 'Pytorch'): 0.0052139020897448065,\n",
       " ((False, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 1536), 'Flash2'): 0.00036150379106402396,\n",
       " ((False, 1, 16, 128, 1536), 'Pytorch'): 0.0008518535085022449,\n",
       " ((False, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 2048), 'Flash2'): 0.0006466096453368663,\n",
       " ((False, 1, 16, 128, 2048), 'Pytorch'): 0.001379361506551504,\n",
       " ((False, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 3072), 'Flash2'): 0.0013255809992551804,\n",
       " ((False, 1, 16, 128, 3072), 'Pytorch'): 0.002942531183362007,\n",
       " ((False, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 6144), 'Flash2'): 0.0050057640857994554,\n",
       " ((False, 1, 16, 128, 6144), 'Pytorch'): 0.010435632187873125,\n",
       " ((False, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 2048), 'Flash2'): 0.0023244195990264416,\n",
       " ((False, 1, 64, 128, 2048), 'Pytorch'): 0.005078200716525316,\n",
       " ((False, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 4096), 'Flash2'): 0.00842154247686267,\n",
       " ((False, 1, 64, 128, 4096), 'Pytorch'): 0.019206266701221466,\n",
       " ((False, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 8192), 'Flash2'): 0.03269826963543892,\n",
       " ((False, 1, 64, 128, 8192), 'Pytorch'): 0.07065707460045814,\n",
       " ((False, 1, 64, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 512), 'Flash2'): 0.00016735635697841643,\n",
       " ((True, 1, 40, 128, 512), 'Pytorch'): 0.0003273654356598854,\n",
       " ((True, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 1024), 'Flash2'): 0.0002902406267821789,\n",
       " ((True, 1, 40, 128, 1024), 'Pytorch'): 0.0009449464082717895,\n",
       " ((True, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 2048), 'Flash2'): 0.0009162109158933163,\n",
       " ((True, 1, 40, 128, 2048), 'Pytorch'): 0.003150817770510912,\n",
       " ((True, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 4096), 'Flash2'): 0.0030188680812716483,\n",
       " ((True, 1, 40, 128, 4096), 'Pytorch'): 0.01186300152912736,\n",
       " ((True, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 8192), 'Flash2'): 0.010886221565306187,\n",
       " ((True, 1, 40, 128, 8192), 'Pytorch'): 0.043682774119079115,\n",
       " ((True, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 1536), 'Flash2'): 0.0001733068749308586,\n",
       " ((True, 1, 8, 128, 1536), 'Pytorch'): 0.0004421956464648247,\n",
       " ((True, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 2048), 'Flash2'): 0.0002290988340973854,\n",
       " ((True, 1, 8, 128, 2048), 'Pytorch'): 0.0007293856702744961,\n",
       " ((True, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 3072), 'Flash2'): 0.00046120647341012954,\n",
       " ((True, 1, 8, 128, 3072), 'Pytorch'): 0.0014542204514145852,\n",
       " ((True, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 6144), 'Flash2'): 0.0014219873398542404,\n",
       " ((True, 1, 8, 128, 6144), 'Pytorch'): 0.005141053330153227,\n",
       " ((True, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 1536), 'Flash2'): 0.000271866824477911,\n",
       " ((True, 1, 16, 128, 1536), 'Pytorch'): 0.000840307530015707,\n",
       " ((True, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 2048), 'Flash2'): 0.0004119739681482315,\n",
       " ((True, 1, 16, 128, 2048), 'Pytorch'): 0.0013607561402022839,\n",
       " ((True, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 3072), 'Flash2'): 0.0008049930259585381,\n",
       " ((True, 1, 16, 128, 3072), 'Pytorch'): 0.0028909499570727347,\n",
       " ((True, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 6144), 'Flash2'): 0.00268098758533597,\n",
       " ((True, 1, 16, 128, 6144), 'Pytorch'): 0.010308381076902151,\n",
       " ((True, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 2048), 'Flash2'): 0.0014050271175801753,\n",
       " ((True, 1, 64, 128, 2048), 'Pytorch'): 0.0050197838805615905,\n",
       " ((True, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 4096), 'Flash2'): 0.004714753646403551,\n",
       " ((True, 1, 64, 128, 4096), 'Pytorch'): 0.01899901308119297,\n",
       " ((True, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 8192), 'Flash2'): 0.01727644031867385,\n",
       " ((True, 1, 64, 128, 8192), 'Pytorch'): 0.06992567382752896,\n",
       " ((True, 1, 64, 128, 8192), 'Triton'): nan}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((False, 1, 40, 128, 512), 'Flash2'): 0.00023322777822613717,\n",
       " ((False, 1, 40, 128, 512), 'Pytorch'): 0.000425503458827734,\n",
       " ((False, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 1024), 'Flash2'): 0.0006006010062992573,\n",
       " ((False, 1, 40, 128, 1024), 'Pytorch'): 0.000992963407188654,\n",
       " ((False, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 2048), 'Flash2'): 0.001444813795387745,\n",
       " ((False, 1, 40, 128, 2048), 'Pytorch'): 0.003181739915162325,\n",
       " ((False, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 4096), 'Flash2'): 0.005307868830859661,\n",
       " ((False, 1, 40, 128, 4096), 'Pytorch'): 0.011935706213116647,\n",
       " ((False, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 40, 128, 8192), 'Flash2'): 0.020440635737031698,\n",
       " ((False, 1, 40, 128, 8192), 'Pytorch'): 0.043976778630167246,\n",
       " ((False, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 1536), 'Flash2'): 0.00019406640902161598,\n",
       " ((False, 1, 8, 128, 1536), 'Pytorch'): 0.00044881651178002357,\n",
       " ((False, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 2048), 'Flash2'): 0.0004031822271645069,\n",
       " ((False, 1, 8, 128, 2048), 'Pytorch'): 0.0007375261560082436,\n",
       " ((False, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 3072), 'Flash2'): 0.0006529037468135357,\n",
       " ((False, 1, 8, 128, 3072), 'Pytorch'): 0.001475837416946888,\n",
       " ((False, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 8, 128, 6144), 'Flash2'): 0.002525606956332922,\n",
       " ((False, 1, 8, 128, 6144), 'Pytorch'): 0.0052139020897448065,\n",
       " ((False, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 1536), 'Flash2'): 0.00036150379106402396,\n",
       " ((False, 1, 16, 128, 1536), 'Pytorch'): 0.0008518535085022449,\n",
       " ((False, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 2048), 'Flash2'): 0.0006466096453368663,\n",
       " ((False, 1, 16, 128, 2048), 'Pytorch'): 0.001379361506551504,\n",
       " ((False, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 3072), 'Flash2'): 0.0013255809992551804,\n",
       " ((False, 1, 16, 128, 3072), 'Pytorch'): 0.002942531183362007,\n",
       " ((False, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((False, 1, 16, 128, 6144), 'Flash2'): 0.0050057640857994554,\n",
       " ((False, 1, 16, 128, 6144), 'Pytorch'): 0.010435632187873125,\n",
       " ((False, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 2048), 'Flash2'): 0.0023244195990264416,\n",
       " ((False, 1, 64, 128, 2048), 'Pytorch'): 0.005078200716525316,\n",
       " ((False, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 4096), 'Flash2'): 0.00842154247686267,\n",
       " ((False, 1, 64, 128, 4096), 'Pytorch'): 0.019206266701221466,\n",
       " ((False, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((False, 1, 64, 128, 8192), 'Flash2'): 0.03269826963543892,\n",
       " ((False, 1, 64, 128, 8192), 'Pytorch'): 0.07065707460045814,\n",
       " ((False, 1, 64, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 512), 'Flash2'): 0.00016735635697841643,\n",
       " ((True, 1, 40, 128, 512), 'Pytorch'): 0.0003273654356598854,\n",
       " ((True, 1, 40, 128, 512), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 1024), 'Flash2'): 0.0002902406267821789,\n",
       " ((True, 1, 40, 128, 1024), 'Pytorch'): 0.0009449464082717895,\n",
       " ((True, 1, 40, 128, 1024), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 2048), 'Flash2'): 0.0009162109158933163,\n",
       " ((True, 1, 40, 128, 2048), 'Pytorch'): 0.003150817770510912,\n",
       " ((True, 1, 40, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 4096), 'Flash2'): 0.0030188680812716483,\n",
       " ((True, 1, 40, 128, 4096), 'Pytorch'): 0.01186300152912736,\n",
       " ((True, 1, 40, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 40, 128, 8192), 'Flash2'): 0.010886221565306187,\n",
       " ((True, 1, 40, 128, 8192), 'Pytorch'): 0.043682774119079115,\n",
       " ((True, 1, 40, 128, 8192), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 1536), 'Flash2'): 0.0001733068749308586,\n",
       " ((True, 1, 8, 128, 1536), 'Pytorch'): 0.0004421956464648247,\n",
       " ((True, 1, 8, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 2048), 'Flash2'): 0.0002290988340973854,\n",
       " ((True, 1, 8, 128, 2048), 'Pytorch'): 0.0007293856702744961,\n",
       " ((True, 1, 8, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 3072), 'Flash2'): 0.00046120647341012954,\n",
       " ((True, 1, 8, 128, 3072), 'Pytorch'): 0.0014542204514145852,\n",
       " ((True, 1, 8, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 8, 128, 6144), 'Flash2'): 0.0014219873398542404,\n",
       " ((True, 1, 8, 128, 6144), 'Pytorch'): 0.005141053330153227,\n",
       " ((True, 1, 8, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 1536), 'Flash2'): 0.000271866824477911,\n",
       " ((True, 1, 16, 128, 1536), 'Pytorch'): 0.000840307530015707,\n",
       " ((True, 1, 16, 128, 1536), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 2048), 'Flash2'): 0.0004119739681482315,\n",
       " ((True, 1, 16, 128, 2048), 'Pytorch'): 0.0013607561402022839,\n",
       " ((True, 1, 16, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 3072), 'Flash2'): 0.0008049930259585381,\n",
       " ((True, 1, 16, 128, 3072), 'Pytorch'): 0.0028909499570727347,\n",
       " ((True, 1, 16, 128, 3072), 'Triton'): nan,\n",
       " ((True, 1, 16, 128, 6144), 'Flash2'): 0.00268098758533597,\n",
       " ((True, 1, 16, 128, 6144), 'Pytorch'): 0.010308381076902151,\n",
       " ((True, 1, 16, 128, 6144), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 2048), 'Flash2'): 0.0014050271175801753,\n",
       " ((True, 1, 64, 128, 2048), 'Pytorch'): 0.0050197838805615905,\n",
       " ((True, 1, 64, 128, 2048), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 4096), 'Flash2'): 0.004714753646403551,\n",
       " ((True, 1, 64, 128, 4096), 'Pytorch'): 0.01899901308119297,\n",
       " ((True, 1, 64, 128, 4096), 'Triton'): nan,\n",
       " ((True, 1, 64, 128, 8192), 'Flash2'): 0.01727644031867385,\n",
       " ((True, 1, 64, 128, 8192), 'Pytorch'): 0.06992567382752896,\n",
       " ((True, 1, 64, 128, 8192), 'Triton'): nan}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.398700341582298e-05\n",
      "0.00018832825124263765\n",
      "nan\n",
      "0.0001949380338191986\n",
      "0.0005280192196369171\n",
      "nan\n",
      "0.00044373203068971635\n",
      "0.0015537716820836066\n",
      "nan\n",
      "0.00178089814260602\n",
      "0.0059799677319824695\n",
      "nan\n",
      "0.007277089785784483\n",
      "0.023374582156538964\n",
      "nan\n",
      "7.451500743627549e-05\n",
      "0.0002345100976526737\n",
      "nan\n",
      "0.00014221351593732834\n",
      "0.00034161580726504325\n",
      "nan\n",
      "0.000214122012257576\n",
      "0.000809457041323185\n",
      "nan\n",
      "0.0008692327700555324\n",
      "0.0030595683120191097\n",
      "nan\n",
      "0.00012004587799310684\n",
      "0.00043376151472330094\n",
      "nan\n",
      "0.0002387201227247715\n",
      "0.0006345830112695693\n",
      "nan\n",
      "0.0004338732920587063\n",
      "0.0015640468709170819\n",
      "nan\n",
      "0.0017551548406481742\n",
      "0.006032583843916655\n",
      "nan\n",
      "0.0007717288658022881\n",
      "0.002488524131476879\n",
      "nan\n",
      "0.0029259537532925605\n",
      "0.009728931300342082\n",
      "nan\n",
      "0.011624757833778858\n",
      "0.037755873911082746\n",
      "nan\n",
      "6.22955895960331e-05\n",
      "0.00027652382850646974\n",
      "nan\n",
      "0.0001347656361758709\n",
      "0.0006315072998404503\n",
      "nan\n",
      "0.00030866755172610284\n",
      "0.0025087839551270006\n",
      "nan\n",
      "0.0010923170484602451\n",
      "0.010051079392433167\n",
      "nan\n",
      "0.004097110796719789\n",
      "0.03983450936153531\n",
      "nan\n",
      "6.884951144456864e-05\n",
      "0.00035479912534356115\n",
      "nan\n",
      "0.0001222752407193184\n",
      "0.0005747934617102146\n",
      "nan\n",
      "0.00016971420496702193\n",
      "0.001374400295317173\n",
      "nan\n",
      "0.000600740872323513\n",
      "0.005292902141809464\n",
      "nan\n",
      "0.00011988669633865356\n",
      "0.0006521859392523765\n",
      "nan\n",
      "0.00016269100829958917\n",
      "0.0010558013059198857\n",
      "nan\n",
      "0.0003065974824130535\n",
      "0.00256704019382596\n",
      "nan\n",
      "0.0010265797562897206\n",
      "0.010014307480305433\n",
      "nan\n",
      "0.0004717133939266205\n",
      "0.00395339461043477\n",
      "nan\n",
      "0.001720838099718094\n",
      "0.01602940207347274\n",
      "nan\n",
      "0.0064673137851059435\n",
      "0.06325799949467183\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for (config, method), t in time_f.items():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 (config, method) 结构转换为分开的多列数据\n",
    "df_time_f = pd.DataFrame([(causal, batch_size, nheads, headdim, seqlen, method, t) \n",
    "                          for ((causal, batch_size, nheads, headdim, seqlen), method), t in time_f.items()], \n",
    "                          columns=['Causal', 'BatchSize','nHeads','HeadDim', 'SeqLen', 'Method', 'Time F'])\n",
    "\n",
    "df_time_b = pd.DataFrame([(causal, batch_size, nheads, headdim, seqlen, method, t) \n",
    "                          for ((causal, batch_size, nheads, headdim, seqlen), method), t in time_b.items()], \n",
    "                          columns=['Causal',  'BatchSize','nHeads','HeadDim', 'SeqLen', 'Method', 'Time B'])\n",
    "df_time_f = df_time_f[df_time_f['Method'] != 'Triton']\n",
    "df_time_b = df_time_b[df_time_b['Method'] != 'Triton']\n",
    "# 保存 DataFrame 到 Excel 文件\n",
    "with pd.ExcelWriter('times10.xlsx') as writer:\n",
    "    df_time_f.to_excel(writer, sheet_name='Forward Times', index=False)\n",
    "    df_time_b.to_excel(writer, sheet_name='Backward Times', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
